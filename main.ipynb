{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a7e3a1-29f8-4097-ae82-b41997dd661b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "DATASET_DIR = r\"C:\\Users\\nadim shah\\OneDrive\\Desktop\\xray classification\\Dataset\"\n",
    "# List of categories corresponding to different disease classes\n",
    "CATEGORIES = [\"Viral Pneumonia\", \"Normal\", \"Lung_Opacity\", \"COVID\"]\n",
    "def load_images_and_masks(dataset_dir, categories):\n",
    "    \"\"\"\n",
    "    Loads images and masks from the dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset_dir (str): Path to the main dataset directory.\n",
    "        categories (list): List of disease categories.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing images and masks for each category.\n",
    "    \"\"\"\n",
    "    data = {category: {\"images\": [], \"masks\": []} for category in categories}\n",
    "    for category in categories:\n",
    "        image_dir = os.path.join(dataset_dir, category, \"images\") \n",
    "        mask_dir = os.path.join(dataset_dir, category, \"masks\")\n",
    "\n",
    "        images = [img for img in os.listdir(image_dir) if img.lower().endswith('.png')]\n",
    "\n",
    "        for img_name in images:\n",
    "            image_path = os.path.join(image_dir, img_name)  \n",
    "            mask_path = os.path.join(mask_dir, img_name)   \n",
    "            \n",
    "            if os.path.exists(mask_path) or os.path.exists(mask_path.replace('.png', '.PNG')):\n",
    "                data[category][\"images\"].append(image_path)\n",
    "                data[category][\"masks\"].append(mask_path if os.path.exists(mask_path) else mask_path.replace('.png', '.PNG'))\n",
    "    \n",
    "    return data \n",
    "\n",
    "data = load_images_and_masks(DATASET_DIR, CATEGORIES)\n",
    "\n",
    "def print_data_lengths(data):\n",
    "    \"\"\"\n",
    "    Prints the count of images and masks for each category.\n",
    "\n",
    "    Args:\n",
    "        data (dict): Dictionary containing images and masks.\n",
    "    \"\"\"\n",
    "    for category, paths in data.items():\n",
    "        images = paths[\"images\"]\n",
    "        masks = paths[\"masks\"]  \n",
    "\n",
    "        print(f\"\\n{category.capitalize()}:\")\n",
    "        print(f\"  Number of images: {len(images)}\")\n",
    "        print(f\"  Number of masks: {len(masks)}\")\n",
    "        print(\"-\" * 50)\n",
    "print_data_lengths(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d13f73-3c44-4d3e-909a-c44ab87b57b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "def balance_images_and_masks(data, viral_pneumonia_count, save_dir):\n",
    "    \"\"\"\n",
    "    Balances the dataset by adjusting the number of images and masks for each category.\n",
    "\n",
    "    Args:\n",
    "        data (dict): Original dataset with images and masks.\n",
    "        viral_pneumonia_count (int): The reference count, typically the number of images in the \n",
    "                                     category with the fewest samples (Viral Pneumonia in this case).\n",
    "\n",
    "    Returns:\n",
    "        dict: Balanced dataset with equalized samples across categories.\n",
    "    \n",
    "    Note:\n",
    "    - Balancing is required because the dataset is unbalanced.\n",
    "    - We use downsampling for categories with more images and upsampling for categories with fewer images.\n",
    "    \"\"\"    \n",
    "    balanced_data = {category: {\"images\": [], \"masks\": []} for category in data.keys()}\n",
    "\n",
    "    for category, paths in data.items():\n",
    "        images = paths[\"images\"]\n",
    "        masks = paths[\"masks\"]\n",
    "        \n",
    "        num_samples = len(images)\n",
    "        if category == \"Viral Pneumonia\":\n",
    "            balanced_data[category][\"images\"] = images\n",
    "            balanced_data[category][\"masks\"] = masks\n",
    "            print(f\"  Kept original number: {num_samples} samples (no change)\")\n",
    "        \n",
    "        elif num_samples > viral_pneumonia_count:\n",
    "\n",
    "            indices = random.sample(range(num_samples), viral_pneumonia_count)\n",
    "            balanced_data[category][\"images\"] = [images[i] for i in indices]\n",
    "            balanced_data[category][\"masks\"] = [masks[i] for i in indices]\n",
    "            print(f\"  Downsampled to: {viral_pneumonia_count} samples\")\n",
    "        \n",
    "        elif num_samples < viral_pneumonia_count:\n",
    "            while len(images) < viral_pneumonia_count:\n",
    "                images.extend(random.sample(images, viral_pneumonia_count - len(images)))\n",
    "                masks.extend(random.sample(masks, viral_pneumonia_count - len(masks)))\n",
    "            balanced_data[category][\"images\"] = images\n",
    "            balanced_data[category][\"masks\"] = masks\n",
    "            print(f\"  Upsampled to: {viral_pneumonia_count} samples\")\n",
    "        \n",
    "        else:\n",
    "\n",
    "            balanced_data[category][\"images\"] = images\n",
    "            balanced_data[category][\"masks\"] = masks\n",
    "            print(f\"  Already balanced at: {num_samples} samples\")\n",
    "        save_category_dir = os.path.join(save_dir, category)\n",
    "        os.makedirs(save_category_dir, exist_ok=True)\n",
    "        \n",
    "        image_save_dir = os.path.join(save_category_dir, \"images\")\n",
    "        mask_save_dir = os.path.join(save_category_dir, \"masks\")\n",
    "        \n",
    "        os.makedirs(image_save_dir, exist_ok=True)\n",
    "        os.makedirs(mask_save_dir, exist_ok=True)\n",
    "\n",
    "        for i, image_path in enumerate(balanced_data[category][\"images\"]):\n",
    "            image_name = f\"{category}_image_{i+1}.jpg\"  \n",
    "            shutil.copy(image_path, os.path.join(image_save_dir, image_name))\n",
    "\n",
    "        for i, mask_path in enumerate(balanced_data[category][\"masks\"]):\n",
    "            mask_name = f\"{category}_mask_{i+1}.jpg\" \n",
    "            shutil.copy(mask_path, os.path.join(mask_save_dir, mask_name))\n",
    "    \n",
    "    return balanced_data\n",
    "\n",
    "def print_balanced_data_length(balanced_data):\n",
    "    \"\"\"\n",
    "    Prints the number of images and masks for each category after balancing.\n",
    "\n",
    "    Args:\n",
    "        balanced_data (dict): Balanced dataset with images and masks.\n",
    "    \"\"\"    \n",
    "    for category, paths in balanced_data.items():\n",
    "        num_images = len(paths[\"images\"])\n",
    "        num_masks = len(paths[\"masks\"])\n",
    "        print(f\"\\nCategory: {category}\")\n",
    "        print(f\"  Number of images after balancing: {num_images}\")\n",
    "        print(f\"  Number of masks after balancing: {num_masks}\")\n",
    "        print(\"-\" * 50)\n",
    "viral_pneumonia_count = len(data[\"Viral Pneumonia\"][\"images\"]) \n",
    "balanced_data = balance_images_and_masks(data, viral_pneumonia_count, save_dir=\"balanced_data_dir\")\n",
    "\n",
    "print_balanced_data_length(balanced_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee2f715-ce92-48e9-abf3-50d798fadded",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "def plot_quadraplot_with_labels(balanced_data, categories):\n",
    "    \"\"\"\n",
    "    Plots a grid of images and their corresponding masks for each category.\n",
    "\n",
    "    Args:\n",
    "        balanced_data (dict): Dictionary containing balanced image and mask paths for each category.\n",
    "        categories (list): List of category names to display.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "    for idx, category in enumerate(categories):\n",
    "\n",
    "        sample_image_path = random.choice(balanced_data[category][\"images\"])\n",
    "        sample_mask_path = random.choice(balanced_data[category][\"masks\"])\n",
    "    \n",
    "        img = Image.open(sample_image_path)\n",
    "        axes[0, idx].imshow(img, cmap=\"gray\")  \n",
    "        axes[0, idx].set_title(f\"{category} - Image\") \n",
    "        axes[0, idx].axis(\"off\") \n",
    "\n",
    "        mask = Image.open(sample_mask_path)\n",
    "        axes[1, idx].imshow(mask, cmap=\"gray\") \n",
    "        axes[1, idx].set_title(f\"{category} - Mask\") \n",
    "        axes[1, idx].axis(\"off\") \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "categories = [\"Viral Pneumonia\", \"Normal\", \"Lung_Opacity\", \"COVID\"]\n",
    "plot_quadraplot_with_labels(balanced_data, categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acbc43b-c59b-49ff-8ce8-a667e3bd8b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Purpose of This Code:\n",
    "- This code is used to increase the size of the dataset by applying augmentation transformations.\n",
    "- For every image in the input directory, this script generates one augmented version per defined transformation.\n",
    "- If you don't want to increase the dataset size, you can skip this part.\n",
    "\n",
    "Key Notes:\n",
    "1. Augmentation ensures that the dataset has more variability, which helps in training a more robust model.\n",
    "2. This script creates one augmented image for each transformation applied to an original image.\n",
    "3. Modify the augmentation_transforms list to add or remove transformations as needed.\n",
    "\"\"\"\n",
    "augmentation_transforms = [\n",
    "    transforms.RandomHorizontalFlip(p=1),  # Flip the image horizontally\n",
    "    transforms.RandomRotation(5),         # Rotate the image by up to 5 degrees\n",
    "]\n",
    "\n",
    "input_folder = r'C:\\Users\\nadim shah\\OneDrive\\Desktop\\xray classification\\balanced_data_dir - Copy'\n",
    "output_folder = r'C:\\Users\\nadim shah\\OneDrive\\Desktop\\xray classification\\augmented_data_dir'\n",
    "\n",
    "for class_name in os.listdir(input_folder):\n",
    "    class_input_path = os.path.join(input_folder, class_name, 'images')\n",
    "    class_output_path = os.path.join(output_folder, class_name, 'images')\n",
    "    if not os.path.isdir(class_input_path):\n",
    "        print(f\"Skipping {class_input_path}, 'images' folder not found.\")\n",
    "        continue\n",
    "    os.makedirs(class_output_path, exist_ok=True)\n",
    "\n",
    "    for image_name in tqdm(os.listdir(class_input_path), desc=f\"Processing {class_name}\"):\n",
    "        image_path = os.path.join(class_input_path, image_name)\n",
    "\n",
    "        if os.path.isfile(image_path):\n",
    "            try:\n",
    "                image = Image.open(image_path).convert(\"RGB\")\n",
    "            except UnidentifiedImageError:\n",
    "                print(f\"Error opening image {image_name}: Unidentified image format.\")\n",
    "                continue\n",
    "            except PermissionError:\n",
    "                print(f\"Error opening image {image_name}: Permission denied.\")\n",
    "                continue\n",
    "            original_image_path = os.path.join(class_output_path, f\"original_{image_name}\")\n",
    "            image.save(original_image_path)\n",
    "\n",
    "            for i, transform in enumerate(augmentation_transforms):\n",
    "                augmented_image = transform(image)\n",
    "                augmented_image_path = os.path.join(class_output_path, f\"augmented_{i}_{image_name}\")\n",
    "                augmented_image.save(augmented_image_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1301d8b-a85b-409b-973e-bf3424ad0e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"\n",
    "Purpose of This Code:\n",
    "- This code handles dataset preparation for training, validation, and testing.\n",
    "- It applies data augmentation transformations to the training dataset to improve model robustness.\n",
    "- Validation and test datasets are resized and normalized without augmentation.\n",
    "- The dataset is split into three parts: training (60%), validation (20%), and testing (20%).\n",
    "\n",
    "Key Notes:\n",
    "1. Augmentation helps create variability in the training dataset to avoid overfitting.\n",
    "2. The transformations include resizing, flipping, rotation, and normalization using ImageNet statistics.\n",
    "3. Validation and test datasets ensure consistency for performance evaluation without any random alterations.\n",
    "4. Data loaders are created to efficiently process data in batches.\n",
    "\"\"\"\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), \n",
    "    transforms.RandomHorizontalFlip(),  \n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize using ImageNet stats\n",
    "])\n",
    "\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  \n",
    "    transforms.ToTensor(),  \n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  \n",
    "])\n",
    "\n",
    "dataset = datasets.ImageFolder(root=r'C:\\Users\\nadim shah\\OneDrive\\Desktop\\xray classification\\augmented_data_dir')\n",
    "\n",
    "train_size = int(0.6 * len(dataset))\n",
    "val_size = int(0.2 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size \n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "train_dataset.dataset.transform = train_transform  \n",
    "val_dataset.dataset.transform = val_test_transform  \n",
    "test_dataset.dataset.transform = val_test_transform  \n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)  \n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)  \n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)  \n",
    "\n",
    "class_labels = dataset.classes \n",
    "print(f\"Class Labels: {class_labels}\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878428e7-19ff-4053-86e7-09371d35333d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print some tensor labels to see which corresponds to which class\n",
    "data_iter = iter(train_loader)\n",
    "images, labels = next(data_iter) \n",
    "\n",
    "for i in range(len(labels)):\n",
    "    print(f\"Image {i}: Label = {labels[i].item()} -> Class = {class_labels[labels[i]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629f5f52-cd84-42c7-ab01-6a8c3a6f59d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchvision import models, transforms\n",
    "\n",
    "\"\"\"\n",
    "Purpose of This Code:\n",
    "- This code sets up a pre-trained VGG16 model eith customize layer for our current desese  classification purpose.\n",
    "- The model is fine-tuned for the specific application by modifying its classifier layers.\n",
    "- Dropout layers are included to prevent overfitting during training.\n",
    "\n",
    "Key Notes:\n",
    "1. VGG16 is chosen for its effectiveness in image classification tasks and its pre-trained weights on ImageNet.\n",
    "2. The convolutional layers are frozen to retain the learned features from the pre-trained model.\n",
    "3. Fully connected layers in the classifier are modified to adapt to the number of classes in the dataset.\n",
    "4. Dropout layers are added to reduce overfitting during training.\n",
    "\"\"\"\n",
    "\n",
    "model = models.vgg16(pretrained=True)\n",
    "\n",
    "for param in model.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Linear(model.classifier[0].in_features, 512),\n",
    "    nn.ReLU(), \n",
    "    nn.Dropout(0.5),  \n",
    "    nn.Linear(512, 256), \n",
    "    nn.ReLU(), \n",
    "    nn.Dropout(0.5), \n",
    "    nn.Linear(256, len(class_labels)) \n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\") \n",
    "model.to(device) \n",
    "\n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "\n",
    "optimizer = Adam(model.classifier.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7d7b9f-3a81-493f-892e-6f2745d6e34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Purpose of This Code:\n",
    "- This code implements the training and validation process for an image classification model.\n",
    "- The model is trained for a specified number of epochs, with learning rate adjustments using a scheduler.\n",
    "- Dropout layers are used in the model to reduce overfitting during training.\n",
    "- Training and validation accuracies are calculated and visualized to monitor model performance.\n",
    "\n",
    "Key Notes:\n",
    "1. **Learning Rate Scheduler**: The StepLR scheduler is used to reduce the learning rate by half every 5 epochs. This gradual reduction helps the model converge more smoothly and avoid overshooting the optimal solution as training progresses.\n",
    "2. **Optimizer**: Adam optimizer is used to update the model parameters, and the learning rate is dynamically adjusted using the StepLR scheduler.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.5)  \n",
    "num_epochs = 10 \n",
    "train_accuracies = [] \n",
    "val_accuracies = [] \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train() \n",
    "    correct_train = 0  \n",
    "    total_train = 0  \n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)  \n",
    "        \n",
    "        optimizer.zero_grad()  \n",
    "        outputs = model(images)  \n",
    "        _, predicted = torch.max(outputs, 1)  \n",
    "\n",
    "        correct_train += (predicted == labels).sum().item() \n",
    "        total_train += labels.size(0) \n",
    "        \n",
    "        loss = criterion(outputs, labels)  \n",
    "        loss.backward()  \n",
    "\n",
    "    train_accuracy = (correct_train / total_train) * 100\n",
    "    train_accuracies.append(train_accuracy)  \n",
    "\n",
    "    model.eval()  \n",
    "    correct_val = 0 \n",
    "    total_val = 0  \n",
    "\n",
    "    with torch.no_grad(): \n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images) \n",
    "            _, predicted = torch.max(outputs, 1) \n",
    "\n",
    "            correct_val += (predicted == labels).sum().item() \n",
    "            total_val += labels.size(0)  \n",
    "\n",
    "    val_accuracy = (correct_val / total_val) * 100\n",
    "    val_accuracies.append(val_accuracy)  \n",
    "\n",
    "\n",
    "    scheduler.step()  \n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Accuracy: {train_accuracy:.2f}%, Val Accuracy: {val_accuracy:.2f}%, Learning Rate: {scheduler.get_last_lr()}\")\n",
    "\n",
    "plt.figure(figsize=(10, 6)) \n",
    "plt.plot(range(1, num_epochs + 1), train_accuracies, label='Training Accuracy', color='blue', marker='o') \n",
    "plt.plot(range(1, num_epochs + 1), val_accuracies, label='Validation Accuracy', color='orange', marker='x')\n",
    "plt.xlabel('Epochs')  \n",
    "plt.ylabel('Accuracy (%)')  \n",
    "plt.title('Training vs. Validation Accuracy') \n",
    "plt.legend(loc='best') \n",
    "plt.grid(True)\n",
    "plt.show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def911fc-bd5a-4a0b-95c6-0e0e29102fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Purpose of This Code:\n",
    "- This code performs evaluation of the model on the test set after the validation phase.\n",
    "\n",
    "- A confusion matrix is generated to visualize the performance of the model for each class.\n",
    "- This helps in understanding how well the model is performing, including the specific classes that may require more attention.\n",
    "\n",
    "Key Notes:\n",
    "1. **Model Evaluation**: The model is set to `eval()` mode to ensure dropout and batch normalization are turned off during inference.\n",
    "2. **Metrics Calculation**:\n",
    "   - The **classification report** provides detailed metrics like precision, recall, and F1-score for each class.\n",
    "   - **Precision** and **recall** are calculated using the macro average, which averages the metrics across all classes without considering class imbalance.\n",
    "   - **F1-score** is the harmonic mean of precision and recall, giving a balance between them.\n",
    "3. **Confusion Matrix**: The confusion matrix is computed and visualized to show how well the model is classifying each class, with annotated counts for true positives, false positives, true negatives, and false negatives.\n",
    "\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "model.eval()  \n",
    "\n",
    "all_labels = [] \n",
    "all_preds = [] \n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images) \n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        all_labels.extend(labels.cpu().numpy()) \n",
    "        all_preds.extend(predicted.cpu().numpy())  \n",
    "\n",
    "print(\"Classification Report (Validation):\")\n",
    "print(classification_report(all_labels, all_preds, target_names=class_labels))\n",
    "\n",
    "accuracy = np.mean(np.array(all_preds) == np.array(all_labels)) \n",
    "precision = precision_score(all_labels, all_preds, average='macro') \n",
    "recall = recall_score(all_labels, all_preds, average='macro')\n",
    "f1 = f1_score(all_labels, all_preds, average='macro') \n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision (Macro): {precision:.4f}\")\n",
    "print(f\"Recall (Macro): {recall:.4f}\")\n",
    "print(f\"F1-Score (Macro): {f1:.4f}\")\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "plt.figure(figsize=(8, 6)) \n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.xlabel('Predicted Labels') \n",
    "plt.ylabel('True Labels') \n",
    "plt.title('Confusion Matrix')  \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89267acc-415f-4f91-ad94-277821d3f0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "\"\"\"\n",
    "Purpose of This Code:\n",
    "- This part of the code implements a pipeline that takes a single image as input, processes it, \n",
    "  and predicts the disease type (e.g., COVID-19, pneumonia, etc.) using a trained model.\n",
    "- The image is preprocessed to match the input format expected by the model, and then the model predicts\n",
    "  the class label based on the processed image.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    \"\"\"\n",
    "    Preprocess the input image to the format expected by the model.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the image file.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: The preprocessed image tensor ready for inference.\n",
    "    \"\"\"\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)), \n",
    "        transforms.ToTensor(), \n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  \n",
    "    ])\n",
    "\n",
    "    image = Image.open(image_path).convert('RGB')  \n",
    "\n",
    "    image = transform(image).unsqueeze(0) \n",
    "    return image\n",
    "\n",
    "def predict_image_class(image_path, model, device):\n",
    "    \"\"\"\n",
    "    Predicts the class label of a single image using the trained model.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the input image.\n",
    "        model (torch.nn.Module): The trained model for classification.\n",
    "        device (torch.device): The device (CPU/GPU) to run the model on.\n",
    "\n",
    "    Returns:\n",
    "        tuple: The predicted class ID and class label.\n",
    "    \"\"\"\n",
    "\n",
    "    image = preprocess_image(image_path)\n",
    "    image = image.to(device) \n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image)\n",
    "        _, predicted = torch.max(outputs, 1)  \n",
    "    \n",
    "    predicted_class = predicted.item() \n",
    "    class_labels = ['covid', 'lung_opacity', 'Normal', 'pneumonia'] \n",
    "    predicted_label = class_labels[predicted_class] \n",
    "    \n",
    "    return predicted_class, predicted_label\n",
    "# Replace with the path of your input image\n",
    "image_path = r\"C:\\Users\\nadim shah\\OneDrive\\Desktop\\xray classification\\balanced_data_dir - Copy\\Normal\\images\\Normal_image_38.jpg\" \n",
    "\n",
    "model.to(device) \n",
    "\n",
    "predicted_class, predicted_label = predict_image_class(image_path, model, device)\n",
    "\n",
    "print(f\"Predicted Class ID: {predicted_class}\")\n",
    "print(f\"Predicted Label: {predicted_label}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
